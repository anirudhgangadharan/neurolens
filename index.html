<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>NeuroLens — Stress & Cognitive Load (Production)</title>
  <style>
    :root{--bg:#071021;--muted:#9fb7d1;--accent:#7dd3fc}
    html,body{height:100%;margin:0;font-family:Inter,system-ui,Segoe UI,Roboto,Arial;background:linear-gradient(180deg,#03101b,#061224);color:#e6eef8}
    .wrap{display:flex;gap:12px;padding:12px;align-items:flex-start}
    #video{display:none}
    #output{position:relative;border-radius:8px;max-width:880px;box-shadow:0 8px 30px rgba(0,0,0,0.6);border:1px solid rgba(255,255,255,0.03)}
    #panel{width:360px;padding:12px}
    .card{background:rgba(255,255,255,0.03);padding:12px;border-radius:8px}
    .row{display:flex;justify-content:space-between;margin:8px 0}
    .small{font-size:13px;color:var(--muted)}
    button{background:var(--accent);color:#022;border:none;padding:8px 10px;border-radius:8px;cursor:pointer}
    button[disabled]{opacity:0.45;cursor:default}
    .metric{font-weight:700;font-size:20px}
    .log{font-size:12px;color:var(--muted);max-height:160px;overflow:auto;margin-top:8px;white-space:pre-wrap}
  </style>
  <!-- MediaPipe CDN scripts -->
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/face_mesh.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js"></script>
</head>
<body>
  <div class="wrap">
    <div>
      <video id="video" autoplay playsinline muted></video>
      <canvas id="output" width="640" height="480"></canvas>
    </div>
    <div id="panel">
      <div class="card">
        <div class="row"><div class="small">Status</div><div id="status" class="small">idle</div></div>
        <div class="row"><div class="small">Blinks / min</div><div id="blinkRate" class="metric">0</div></div>
        <div class="row"><div class="small">PERCLOS (60s)</div><div id="perclos" class="metric">0%</div></div>
        <div class="row"><div class="small">Avg blink (ms)</div><div id="avgBlink" class="metric">0</div></div>
        <div class="row"><div class="small">Brow tension</div><div id="browTension" class="metric">0</div></div>
        <div class="row"><div class="small">Lip tension</div><div id="lipTension" class="metric">0</div></div>
        <div class="row"><div class="small">Stress level</div><div id="stressLevel" class="metric">0</div></div>
        <div class="row"><div class="small">Cognitive load</div><div id="cogLoad" class="metric">0</div></div>
        <div style="margin-top:10px;display:flex;gap:6px">
          <button id="startBtn">Start</button>
          <button id="stopBtn" disabled>Stop</button>
        </div>
        <div class="log" id="log">Ready.</div>
      </div>
    </div>
  </div>

<script>
// NeuroLens — Production-ready HTML
// Improvements/fixes: corrected log newline, more robust browIndex, start/stop UI state, gentle error handling.

const video = document.getElementById('video');
const canvas = document.getElementById('output');
const ctx = canvas.getContext('2d');
const startBtn = document.getElementById('startBtn');
const stopBtn = document.getElementById('stopBtn');
const logEl = document.getElementById('log');
const statusEl = document.getElementById('status');

// UI fields
const blinkRateEl = document.getElementById('blinkRate');
const perclosEl = document.getElementById('perclos');
const avgBlinkEl = document.getElementById('avgBlink');
const browTensionEl = document.getElementById('browTension');
const lipTensionEl = document.getElementById('lipTension');
const stressEl = document.getElementById('stressLevel');
const cogEl = document.getElementById('cogLoad');

// landmark index groups (MediaPipe FaceMesh)
const LEFT_EYE = [33,160,158,133,153,144];
const RIGHT_EYE = [362,385,387,263,373,380];
const LEFT_IRIS = [468,469,470,471];
const RIGHT_IRIS = [473,474,475,476];
const MOUTH_TOP = 13, MOUTH_BOTTOM = 14;

// state
let running = false;
let blinkEvents = []; // {t,duration}
let closureEpoch = null; // {start}
let closureDurations = []; // closed durations in ms for PERCLOS
let smoothStress = null, smoothCog = null;
let gazeHistory = [];
let cam = null;

function log(msg){
  const ts = '[' + new Date().toLocaleTimeString() + '] ';
  logEl.textContent = ts + msg + "\n" + logEl.textContent;
}

function mean(arr){ if(!arr || arr.length===0) return 0; return arr.reduce((a,b)=>a+b,0)/arr.length; }
function distance(a,b){ const dx=a.x-b.x, dy=a.y-b.y; return Math.sqrt(dx*dx+dy*dy); }
function meanPoint(indices, landmarks){ const pts = indices.map(i=>landmarks[i]); const s = pts.reduce((a,b)=>({x:a.x+b.x,y:a.y+b.y}),{x:0,y:0}); return {x:s.x/pts.length, y:s.y/pts.length}; }
function toScreen(pt){ return { x: pt.x * canvas.width, y: pt.y * canvas.height }; }
function eyeAspectRatio(indices, landmarks){ const p = indices.map(i=>landmarks[i]); const p1=p[0], p2=p[1], p3=p[2], p4=p[3], p5=p[4], p6=p[5]; const v1 = Math.hypot(p2.y-p6.y, p2.x-p6.x); const v2 = Math.hypot(p3.y-p5.y, p3.x-p5.x); const h = Math.hypot(p1.y-p4.y, p1.x-p4.x); if(h===0) return 0; return (v1+v2)/(2.0*h); }

function browIndex(side){
  // side: 0 -> left-inner, 1 -> right-inner, 2 -> left-outer, 3 -> right-outer
  switch(side){
    case 0: return 63; case 1: return 293; case 2: return 105; case 3: return 334;
    default: return 63;
  }
}

function analyzeFace(landmarks){
  const leftEyeEAR = eyeAspectRatio(LEFT_EYE, landmarks);
  const rightEyeEAR = eyeAspectRatio(RIGHT_EYE, landmarks);
  const ear = (leftEyeEAR + rightEyeEAR) / 2;
  const now = performance.now();

  const blinkThreshold = 0.22;
  const isClosed = ear < blinkThreshold;
  if(isClosed && !closureEpoch){ closureEpoch = {start: now}; }
  if(!isClosed && closureEpoch){
    const dur = now - closureEpoch.start; closureDurations.push({t: now, d: dur});
    if(dur < 1000) { blinkEvents.push({t: now, d: dur}); }
    closureEpoch = null;
  }

  const windowMs = 60000;
  blinkEvents = blinkEvents.filter(b => now - b.t <= windowMs);
  closureDurations = closureDurations.filter(c => now - c.t <= windowMs);

  const blinkRate = blinkEvents.length; // per 60s
  const avgBlinkDur = blinkEvents.length ? Math.round(mean(blinkEvents.map(b=>b.d))) : 0;
  const totalClosed = closureDurations.reduce((a,b)=>a+b.d,0) + (closureEpoch ? (now - closureEpoch.start) : 0);
  const perclos = Math.round((totalClosed / windowMs) * 100);

  // Brow tension: vertical distance between eyebrow inner point and eye center
  const leftBrowInner = landmarks[browIndex(0)];
  const rightBrowInner = landmarks[browIndex(1)];
  const leftEyeCenter = meanPoint([33,133,159,145], landmarks);
  const rightEyeCenter = meanPoint([362,263,386,373], landmarks);
  const browTensionLeft = (leftBrowInner.y - leftEyeCenter.y);
  const browTensionRight = (rightBrowInner.y - rightEyeCenter.y);
  const browTension = Math.max(0, (browTensionLeft + browTensionRight)/2);

  // Lip tension: inverse of mouth openness normalized to face width
  const upper = landmarks[MOUTH_TOP]; const lower = landmarks[MOUTH_BOTTOM];
  const cheekL = landmarks[234] || {x:0,y:0}; const cheekR = landmarks[454] || {x:1,y:0};
  const faceWidth = distance(cheekL, cheekR) || 0.0001;
  const mouthOpen = distance(upper, lower) / faceWidth;
  const lipTension = Math.max(0, Math.min(1, 1 - (mouthOpen / 0.06)));

  // gaze dynamics (rough)
  const leftIris = meanPoint(LEFT_IRIS, landmarks);
  const rightIris = meanPoint(RIGHT_IRIS, landmarks);
  const leftEyeBox = meanPoint([33,133,159,145], landmarks);
  const rightEyeBox = meanPoint([362,263,386,373], landmarks);
  const gazeX = ((leftIris.x - leftEyeBox.x) + (rightIris.x - rightEyeBox.x)) / 2;
  const gazeY = ((leftIris.y - leftEyeBox.y) + (rightIris.y - rightEyeBox.y)) / 2;
  const screenGaze = toScreen({x: gazeX, y: gazeY});

  const last = gazeHistory.length ? gazeHistory[gazeHistory.length-1] : null;
  const vel = last ? Math.hypot(screenGaze.x - last.x, screenGaze.y - last.y) : 0;
  gazeHistory.push({t: now, x: screenGaze.x, y: screenGaze.y, v: vel});
  while(gazeHistory.length && now - gazeHistory[0].t > 2000) gazeHistory.shift();
  const avgVel = gazeHistory.length ? mean(gazeHistory.map(g=>g.v)) : 0;

  // Cognitive load heuristic
  const baselineBlink = 15;
  const blinkFactor = Math.max(0, (baselineBlink - blinkRate) / baselineBlink);
  const fixFactor = Math.max(0, Math.min(1, (1 - (avgVel / 30))));
  const browFactor = Math.max(0, Math.min(1, browTension * 5));
  let cog = (0.5 * blinkFactor) + (0.35 * fixFactor) + (0.15 * browFactor);
  cog = Math.max(0, Math.min(1, cog));

  // Stress heuristic
  const blinkDeviation = Math.abs(blinkRate - baselineBlink) / baselineBlink;
  const avgBlinkNorm = Math.max(0, Math.min(1, avgBlinkDur / 300));
  let stress = (0.4 * browFactor) + (0.25 * blinkDeviation) + (0.2 * lipTension) + (0.15 * avgBlinkNorm);
  stress = Math.max(0, Math.min(1, stress));

  if(smoothCog === null) smoothCog = cog; else smoothCog = smoothCog * 0.6 + cog * 0.4;
  if(smoothStress === null) smoothStress = stress; else smoothStress = smoothStress * 0.6 + stress * 0.4;

  const stressPct = Math.round(smoothStress * 100);
  const cogPct = Math.round(smoothCog * 100);

  // update UI
  blinkRateEl.textContent = blinkRate;
  perclosEl.textContent = perclos + '%';
  avgBlinkEl.textContent = avgBlinkDur;
  browTensionEl.textContent = (browTension.toFixed(3));
  lipTensionEl.textContent = (lipTension.toFixed(2));
  stressEl.textContent = stressPct;
  cogEl.textContent = cogPct;

  return {ear, blinkRate, perclos, avgBlinkDur, browTension, lipTension, stressPct, cogPct, avgVel};
}

// Initialize FaceMesh
const faceMesh = new FaceMesh({ locateFile: (file) => `https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh/${file}` });
faceMesh.setOptions({ maxNumFaces: 1, refineLandmarks: true, minDetectionConfidence: 0.6, minTrackingConfidence: 0.5 });

faceMesh.onResults(results => {
  if(!video.videoWidth) return;
  canvas.width = video.videoWidth; canvas.height = video.videoHeight;
  ctx.save(); ctx.clearRect(0,0,canvas.width,canvas.height); ctx.drawImage(results.image, 0, 0, canvas.width, canvas.height);

  if (results.multiFaceLandmarks && results.multiFaceLandmarks.length > 0) {
    const landmarks = results.multiFaceLandmarks[0];
    // draw sparse landmarks
    ctx.fillStyle = 'rgba(125,211,252,0.85)';
    for(let i=0;i<468;i+=4){ const p = toScreen(landmarks[i]); ctx.beginPath(); ctx.arc(p.x, p.y, 1.2, 0, Math.PI*2); ctx.fill(); }

    const hs = analyzeFace(landmarks);
    ctx.fillStyle = 'rgba(255,255,255,0.95)'; ctx.font = '16px Arial'; ctx.fillText(`Stress: ${hs.stressPct}%`, 10, 24); ctx.fillText(`CogLoad: ${hs.cogPct}%`, 10, 44);
    statusEl.textContent = 'running';
  } else {
    statusEl.textContent = 'no face detected';
  }
  ctx.restore();
});

async function startCamera(){
  if(running) return;
  startBtn.disabled = true; stopBtn.disabled = false;
  try{
    const stream = await navigator.mediaDevices.getUserMedia({ video: { width: 1280, height: 720, facingMode: 'user' }, audio: false });
    video.srcObject = stream; await video.play();
    cam = new Camera(video, { onFrame: async () => { await faceMesh.send({ image: video }); }, width: 1280, height: 720 });
    cam.start(); running = true; statusEl.textContent = 'running'; log('Camera started');
  }catch(e){
    startBtn.disabled = false; stopBtn.disabled = true;
    statusEl.textContent = 'error';
    log('Camera error: ' + (e.message || e));
    alert('Camera access failed: ' + (e.message || e));
  }
}

function stopCamera(){
  if(!running) return;
  try{
    if(cam) cam.stop();
    const s = video.srcObject; if(s) s.getTracks().forEach(t=>t.stop()); video.srcObject = null;
  }catch(e){ console.warn(e); }
  running = false; startBtn.disabled = false; stopBtn.disabled = true; statusEl.textContent = 'stopped'; log('Camera stopped');
}

startBtn.addEventListener('click', startCamera);
stopBtn.addEventListener('click', stopCamera);

log('Production build loaded. Click Start to begin.');
</script>
</body>
</html>
